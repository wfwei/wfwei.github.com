---
layout: post
title: "word segmentation"
category: posts
---

中文分词，目前主要有两种思路：基于字典，使用最大匹配算法；基于统计，研究词的统计特征

1. 基于字典的分词技术
    * 中科院分词系统(ICTCLAS)--基于分层隐马尔可夫的分词系统

2. 基于统计的分词技术
    * 现在比较流行的是基于条件随机场(CRF)，以后有机会再研究，[参考](http://blog.csdn.net/marising/article/details/4083650)
    * 比较经典的一个方法是：[MMSEG(Max Match Segmentation)](http://technology.chtsai.org/mmseg/) 我以前看博客的简单理解和实现
        1. 考虑问题的本源，怎样的‘字串’可以视为‘词’？
            * 字串出现的频率
            * 字串内部性质，‘子串’间的凝固程度
                * 凝固程度是两个子串同时出现的频率
                * 字串AB, 其子串为A和B，凝固程度为 `W = P(AB) / (P(A)*P(B))` W越大，凝固程度越高，AB越有可能是一个词
            * 字串外部性质，字串使用环境的自由程度，表现为相邻字集合是否丰富且随机，使用信息熵来衡量
                * 字串自由独立，能灵活出现在不同环境中，相邻字集合丰富且随机
                * 使用信息熵衡量字串左右邻字集合随机程度
        2. [Java实现](https://github.com/wfwei/MyWordSeg)

>在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。
>把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对“四是四十是十十四是十四四十是四十”的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置


