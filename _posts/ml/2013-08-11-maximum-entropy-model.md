---
layout: post
title: "最大熵模型"
categories: [ml, entropy]
---

最大熵模型是最大熵原理在**分类问题**上的应用

###最大熵原理：

学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型

直观的，最大熵原理认为，概率模型首先要满足已有约束，对于不确定部分，做‘等可能’约束

###最大熵模型

将最大熵原理应用到分类问题就得到了最大熵模型

假设分类模型是一个条件概率分布： $$ P(Y | X) $$ ，其中，X表示输入，Y表示输出(类别)，该模型接受输入X，通过 $$P(Y | X)$$ ，输出Y

对于训练数据集合 $$T={(x_1 , y_1 ),...,(x_n , y_n )}$$ ，学习的目标是寻找最好的分类模型 $$P(Y|X)$$

假设描述数据真实模型的特征函数为$$f(x,y)$$，只有当x的类别是y的时候有$$f(x,y)=1$$，否则$$f(x,y)=0$$

特征函数$$f(x,y)$$关于经验分布$$\tilde{P}(X,Y)$$的期望为： $$E_\tilde{p}(f) = \sum_{x,y} \tilde{P}(x,y)f(x,y) $$

特征函数$$f(x,y)$$关于模型$$P(Y|X)$$以及经验分布$$\tilde{P}(X)$$的期望为： $$E_p(f) = \sum_{x,y} \tilde{P}(x)P(y|x)f(x,y) $$

假如模型$$P(Y|X)$$能正确描述数据，则有： $$E_\tilde{p} (f) = E_p (f)$$

因此，最大熵模型的学习等价于下面优化问题：

$$
\begin{align}
\max_{P \in C} &~~ H(P)=-\sum_{x,y} \tilde{P}(x)P(Y|X)logP(y|X) \\
s.t. &~~ E_\tilde{p}(f) = E_p(f) \\
     &~~ \sum_y P(Y|X) = 1
\end{align}
$$

上面公式中，经验分布(概率)是指给定数据集上的统计分布(概率)，所以都是常量

这里，可以将最优化原始问题转化为无约束最优化的对偶问题

### 极大似然估计

已知训练数据的经验概率分布$$P(X|Y)$$，条件概率分布$$P(Y|X)$$的对数似然函数表示为：

$$
L_P (P) = \sum_{x,y} P(x)*P(y|x)logP(x|y)
$$
