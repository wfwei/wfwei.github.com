---
layout: post
title: "最大熵模型"
categories: [ml, entropy]
---

TODO　修改经验××的字符表示

###最大熵原理：
学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型

直观的，最大熵原理认为，概率模型首先要满足已有约束，对于不确定部分，做‘等可能’约束

###最大熵模型

将最大熵原理应用到__分类问题__就得到了最大熵模型

假设分类模型是一个条件概率分布：P(Y|X)，其中，X表示输入，Y表示输出(类别)，该模型接受输入X，通过P(Y|X)，输出Y

对于训练数据集合 T={(x_1, y_1),...,(x_n, y_n)} ，学习的目标是寻找最好的分类模型P(Y|X)

假设描述数据真实模型的特征函数为f(x,y)，只有当x的类别是y的时候有f(x,y)=1，否则f(x,y)=0

特征函数f(x,y)关于经验分布$$P(X,Y)$$的期望为： $$E_p(f) = \sum_{x,y} P(x,y)f(x,y) $$

特征函数f(x,y)关于模型P(Y|X)以及经验分布$$P(X)$$的期望为： $$E_p(f) = \sum_{x,y} P(x)P(y|x)f(x,y) $$

假如模型P(Y|X)能正确描述数据，则有： $$E_p(f) = E_p(f)$$

因此，最大熵模型的学习等价于下面优化问题：

$$
\begin{align}
\max_{P in C} &~~ H(P)=-\sum_{x,y} P(x)P(Y|X)logP(y|X) \\
s.t. &~~ E_p(f) = E_p(f) \\
     &~~ \sum_y P(Y|X) = 1
\end{align}
$$

上面公式中，经验分布(概率)是指给定数据集上的统计分布(概率)，所以都是常量

这里，可以将最优化原始问题转化为无约束最优化的对偶问题

### 极大似然估计
已知训练数据的经验概率分布P(X|Y)，条件概率分布P(Y|X)的对数似然函数表示为：

$$
L_P (P) = \sum_{x,y} P(x)*P(y|x)logP(x|y)
$$
