---
layout: post
title: "贝叶斯理论通俗解释及应"
categories: [ml, classification, bayes]
---

##Bayes方法就是一个‘逆概’问题

1. Wiki例子

    一所学校里面有 60% 的男生，40% 的女生。男生总是穿长裤，女生则一半穿长裤一半穿裙子。有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近似，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？

2. Bayes公式：

        P(Girl|Pants) = 穿长裤的人中，女生的比例 = 穿长裤的女生数 / 穿长裤的人数
                      = P(Girl) * P(Pants|Girl) / [P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)]
        一般形式为：
        P(B|A) = P(A|B) * P(B) / [P(A|B) * P(B) + P(A|~B) * P(~B) ]
               = P(A|B) * P(B) / P(A)
               = P(AB)/P(A)

3. bayes公式结合了先验概率和极大似然，合二为一
    * 极大似然是以当前情况为中心，以当前情况最有可能发生为目标进行建模和预测
    * 先验概率则是考虑了全局(历史)情况，统计了每种预测的全局概率
    * 以拼写纠正为例，极大似然就是根据用户输入D，找最相似的h进行猜测，即计算p(h|D)
    * 但是，有些猜测h出现的几率非常小，简单利用p(h|D)猜测用户想输入h显然是不合理的
    * 比如用户输入tlp，根据极大似然，我们找到编辑距离最近的两个单词top和tip，如何抉择？
    * 一个更好的解释单纯极大似然不合理的例子是曲线拟合
        * 给几个数字-1, 3, 7, 11，这个数列可以是等差数列的点，也可以是-X^3 / 11 + 9/11*X^2 + 23/11
        的点
        * 极大似然可能认为是后者更好的满足了输入序列，但实际中，这往往形成‘过拟合’，
        * 一般来说，越低阶的多项式越靠谱（当然是要考虑似然的前提下），其中一个原因‘可能’就是低阶多项式更常见?
    * 我们的观测数据总会有各种误差，无论是观测误差或系统误差，如果过分拟合观测数据，就肯定会失真。
    * 观测数据的成分复杂，由各种因素集体贡献。一个现实的模型往往只提取出几个与结果相关度很高的因素。

4. 应用例子

    1. 拼写纠正问题

        设用户输入单词为D，猜测其想输入h，我们的目标就是求最大的P(h|D)  
        运用Bayes得到: `p(h|D)=P(h)*P(D|h)/P(D)` 
        由于不同的猜测h1,h2...,P(D)是一样的，所以得到：  
        `p(h|D)~P(h)*P(D|h)`   
        这个公式解读为：对于用户输入D，猜测h的好坏取决与猜测h本身的先验概率p(h)，以及h和D的似然概率的乘积  
        具体的做法: 先验概率p(h)可以根据用户输入历史数据直接统计得到；P(D|h)可以计算单词的编辑距离或键盘布局导致的误输入可能性得到  
        给定用户输入D，找h的时候，可以事先为每个单词建立一个倒排索引，比如存放每个单词编辑距离小于2的单词 
    
    2. 中文分词

        设X为字串（句子），Y为词串，我们的目标就是求得使P(Y|X)最大的Y，使用bayes得到  
        `P(Y|X)~P(X|Y)*P(Y)`  
        这个公式解读为，这种分词的正确性正比与分词后词串的可能性P(Y)，以及词串到原始句子的转换可能性P(X|Y)，一般来说，P(X|Y)是接近1的，可以不考虑，所以关键就是最大化P(Y)，即计算一个词串的可能性:
        `P(Y)=P(W1, W2, W3...)=P(W1)*P(W2|W1)*P(W3|W1,W2)...`  
        使用n-gram原理，认为只有连续的n个词之间有相互影响，极端情况下，认为所有的词都是独立的（马尔可夫性质, 1-gram）:  
        `P(Y)=P(W1)*P(W2)*P(W3)...`  
        当然用2-gram可能要好点  

    3. 统计机器翻译

        设e为句子，f为翻译后的句子，想在要求哪个f最靠谱，即P(f|e)最大，贝叶斯之:
        `P(f|e) ~ P(f) * P(e|f)`  
        这个公式的解读为，翻译的准确性正比于翻译后句子的先验概率P(f)，以及f到e的转化概率P(e|f)。P(f)可以使用n-gram模型简单计算，但是p(e|f)就比较棘手了，我们首先要将e和f对齐，及每个词的对应关系，有了这个对应关系，就很容易求出p(e|f)，比如:
        e:I love you f:我爱你  
        对其: I(我) love(爱) you(你)  
        p(e|f)=P(I|我)*P(love|爱)*P(you|你)  
        所以关键是如何对其，我们需要一个分词对齐的平行语料库，TODO NLP基础书已经下载

    4. 垃圾邮件过滤

        设D为一封邮件，h+/h-分别代表垃圾邮件和非垃圾邮件，需要计算和比较：
        `P(h+|D) ~ P(D|h+)*P(h+)` 和 `P(h-|D) ~ P(D|h-)*P(h-)`    
        P(h+)和P(h-)两个先验概率很容易从训练集中统计出来，如何计算P(D|h+)和P(D|h-)，以P(D|h+)为例：
        D由N个词组成，W1,W2,W3...，因此:  
        `P(D|h+)=P(W1,W2,W3...|h+)=P(W1|h+)*P(W2|W1,h+)*P(W3|W2,W1,h+)...`  
        这个数据太稀疏了，这是在垃圾邮件中找完全相同的一封邮件  
        使用朴素贝叶斯，认为每个词都没有关联（1-gram）简化得到:  
        `P(D|h+)=P(W1|h+)*P(W2|h+)*P(W3|h+)...`  
        即简单统计每个词在垃圾邮件中出现的词频，Easy done

## 其他
* 奥卡姆剃刀，将简约，经济的思想用于逻辑和问题解决，在几个差不多的猜测中，选取假设较少的
* 奥卡姆剃刀精神：如果两个理论具有相似的解释力度，那么优先选择那个更简单的（往往也正是更平凡的，更少繁复的，更常见的）
* 统计学家vs贝叶斯学家
  1. 统计学家：让数据自己说话，强调使用极大似然
  2. ‘贝叶斯学家’：数据本身会有偏差，一个合理的先验概率可以是模型健壮(先验概率也是统计数据)
* 当缺失先验概率或先验概率分布均匀时，就只能使用似然估计了
* 所谓的推理，分为两个过程，第一步是对观测数据建立一个模型。第二步则是使用这个模型来推测未知现象发生的概率。
* 我们前面都是讲的对于观测数据给出最靠谱的那个模型。然而很多时候，虽然某个模型是所有模型里面最靠谱的，但是别的模型也并不是一点机会都没有。譬如第一个模型在观测数据下的概率是 0.5 。第二个模型是 0.4 ，第三个是 0.1
* 。如果我们只想知道对于观测数据哪个模型最可能，那么只要取第一个就行了，故事到此结束。然而很多时候我们建立模型是为了推测未知的事情的发生概率，这个时候，三个模型对未知的事情发生的概率都会有自己的预测，仅仅因为某一个模型概率稍大一点就只听他一个人的就太不民主了。所谓的最优贝叶斯推理就是将三个模型对于未知数据的预测结论加权平均起来（权值就是模型相应的概率）。显然，这个推理是理论上的制高点，无法再优了，因为它已经把所有可能性都考虑进去了。TODO全都考虑就一定好？
* 最大似然和最小二乘 可以使用bayes证明合理性
* 朴素贝叶斯 假设条件都独立，1-gram，马尔可夫性质的贝叶斯  

*[REF](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/) 
