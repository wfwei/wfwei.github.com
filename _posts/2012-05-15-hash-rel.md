---
layout: post
title: "哈希（散列）Hashing"
categories: posts 
---

## 普通哈希和大数据下的哈希
1. 哈希表（Hash table，也叫散列表）
  1. 根据关键码值(Key value)而直接进行访问的数据结构
  2. 冲突解决策略
    * Chaining: 当多个不同的key被映射到相同的slot时，chaining方式采用链表保存所有的value
    * Open addressing: 在该slot的邻近位置查找，直到找到对应的value或者空闲的slot， 这个过程被称作探测(probing)。常见的探测策略有Linear probing，Quadratic probing和Double hashing。


2. 在大数据下，比如有份文件，包含10亿个int数字，找出重复出现过的数字，而内存限制是32M，如何做？
  1. int的范围是[-2^31 , 2^31-1 ]，共2^32 -1 个，int的大小是4字节，那么，如果使用普通哈希的话，需要内存:(2^32 -1)*4B ~ 16GB,这对于普通机器是无法容忍的
  2. 数据都是int类型的，比较容易实现完美哈希(即每个int值直接模上哈希表长度作为其在哈希表中的位置)，所以可以使用一个比特位来标识这个数字是否以出现过(类似基数排序的思想)，如果使用位图来存，存下所有int数字还是需要:(2^32-1)bite ~ 512M,依然太大了
  3. 32M内存，我们拿出16M作为哈希，使用位图可以表示128M个数字，我们将int数从小到大切分，每份128M个数字，共2^5 =32份，这样一份份比较，就把问题解决了（桶排序的思想），具体说，把大文件切分成32个小文件，第i个文件中存放[(i-1)*2^27 ~i*2^27]之间的数字,之后分别统计每个文件中是否有重复数字即可～
  4. 这个题目还有另外一个思路，把int范围内的数做成哈希，显然需要的内存太大了，想到布隆过滤器就是专门解决这种问题的，布隆过滤器在保证误判率较低的情况下大大减小哈系空间

##Perfect hashing
完美哈希,没有冲突的哈希函数.也就是，函数 H 将 N 个 KEY 值映射到 M 个整数上，这里 M>=N ，而且，对于任意的 KEY1 ，KEY2 ，H( KEY1 ) != H( KEY2 ) ，并且，如果 M = = N ，则 H 是最小完美哈希函数（Minimal Perfect Hash Function，简称MPHF)  
完美哈希是针对**静态集合**的，即所有的key都是事先已知且固定的。  
完美哈希有多种代码生成器，根据key集合自动生成哈希函数，如[gperf](http://www.gnu.org/software/gperf/)

## Locality-Sensitive Hashing(LSH)
    
**核心思想**:   
将相似输入（key）以高概率散列到同一个桶（bucket）中

**应用**  

1. probabilistic dimension reduction of high-dimensional data(降维)
2. ‘近似’最近邻算法 
	
**几种实现**    

1. Bit sampling for Hamming distance
2. *Min-wise independent permutations*
3. *Random projection*
4. Stable distributions

**min-wise independent permutations**
[TODO](http://en.wikipedia.org/wiki/MinHash)
		
**Random Projection(随机映射)**  
随机映射的相似性哈希算法比较的是两个向量的余弦距离。基本思想是随机构造k个平面(hyperplane)，每个平面都有其法向量表示，k个平面将整个空间且分为f(k)个子空间。
		
1. 选取k个hyperplane，构成2^k个bucket
2. 每个hyperplane都从高斯分布（比如N(0,1)）中random出一个向量u
3. 对于每个document，提取feature，构成一个向量v
4. 计算h(v)=v*u,通过sgn(h(v))的符号判断v在u所代表平面的哪一侧。按照这样的策略，两个document x,y被分到同一侧（认为collision）的概率是：`p= Pr(h(x)=h(y)) = 1-theta(x,y)/PI ---- theta(x,y) is the angle between x and y`
5. 每个document都在k个平面上利用4中方法计算，形成一次映射projection. 一次映射后，x,y被映射到同一个bucket的概率是p’=p^k，指数速度变小了
6. 一次映射后，p’是很小的，这很可能把locality-sensitive的点分到不同的bucket中
7. 可以做L次映射，只要在一次映射中最近邻被分到同一个bucket就可以了
8. 在给定平面数目k和最近邻被分到不同bucket的期望概率w的情况下，可以得到：`(1-p’)^L = w`
	    
**注意点：**  

1. k的大小影响到粒度，需要实际调节
2. 1-theta(x,y)/PI 和 cos(x,y) 的关系使得本方法中，locality-sensitive中的距离是指两个vector的cosin
3. w的大小影响到hash的效果，需实际调节

**参考:**

* [LSH@wikipedia](http://en.wikipedia.org/wiki/Locality-sensitive_hashing)
* [locality sensitive hashing for finding nearest neighbors](https://scholar.google.com)
* [Streaming First Story Detection with application to Twitter](https://scholar.google.com)

