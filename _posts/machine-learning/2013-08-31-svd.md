---
layout: post
title: "特征值和奇异值分解"
categories: [ml, nlp]
---

这篇笔记主要参考LetfNotEasy的[博文](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)

TODO 应用 PCA LSI 

###特征值及分解

对于**方阵**$$A_{n*n}$$，其特征值$$\lambda$$和特征向量$$\vec{v}$$满足下面条件：

$$ A*\vec{v}_i = \lambda_i*\vec{v}_i $$

设方阵A有m个非零特征值，对应的特征向量分别为: $$\vec{v}_1, \vec{v}_2,...,, \vec{v}_m$$

$$
\begin{aligned}
A\begin{bmatrix} \vec{v}_1 & \vec{v}_2 & ... & \vec{v}_m \end{bmatrix}
&= \begin{bmatrix} A\vec{v}_1 & A\vec{v}_2 & ... & A\vec{v}_m \end{bmatrix}\\
&= \begin{bmatrix} \lambda_1 \vec{v}_1 & \lambda_2 \vec{v}_2 & ... & \lambda_m \vec{v}_m \end{bmatrix}\\
&= \begin{bmatrix} \vec{v}_1 & \vec{v}_2 & ... & \vec{v}_m \end{bmatrix}*\begin{bmatrix} \lambda_1 &  &  & \\   &  \lambda_2 &  & \\  &  &  ... & \\  &  &  & \lambda_m\end{bmatrix} 
\end{aligned}
$$

设特征向量矩阵为$$Q=\begin{bmatrix} \vec{v}_1 & \vec{v}_2 & ... & \vec{v}_m \end{bmatrix}$$，特征值对角阵为$$\Sigma = \begin{bmatrix} \lambda_1 &  &  & \\   &  \lambda_2 &  & \\  &  &  ... & \\  &  &  & \lambda_m\end{bmatrix} $$ 

当**m=n**时，也即方阵A非奇异时，也即方阵A可逆时，A有n个特征向量和特征值，又由于特征向量之间两两正交，所以特征矩阵Q可逆，得：

$$A=Q*\Sigma*Q^{-1}=Q*\Sigma*Q^T$$

为了降维或抽取重要特征，我们可以选取k个最大的特征值及对应的特征向量，则有:

$$
\begin{aligned} 
A  & \sim \begin{bmatrix} \vec{v}_1 & \vec{v}_2 & ... & \vec{v}_k \end{bmatrix}*\begin{bmatrix} \lambda_1 &  &  & \\   &  \lambda_2 &  & \\  &  &  ... & \\  &  &  & \lambda_k\end{bmatrix}  \begin{bmatrix} \vec{v}_1 \\ \vec{v}_2 \\ ... \\ \vec{v}_k \end{bmatrix}\\
&=Q_{n*k} \Sigma_kQ_{n*k}^T
\end{aligned}
$$


###奇异值及分解

特征值分解只能针对**方阵**，实际问题中大部分都不是方针，奇异值分解就是解决非方阵的矩阵分解问题

设矩阵A为m*n的矩阵$$A_{m*n}$$，奇异值分解是通过将A乘以其转置$$A^T$$，转化为方阵，通过求解方阵的特征值和特征向量，再转化为矩阵A的‘特征值’‘特征向量’，即奇异值奇异向量

根据$$(A^TA)\vec{v}_i = \lambda_i \vec{v}_i$$，求得方阵$$A^TA$$的特征值$$\lambda$$和特征向量$$\vec{v}$$，则矩阵A的奇异值和奇异向量分别为：

$$
\begin{aligned}
\sigma &= \sqrt{\lambda} \\
\mu &= \frac{A\vec{v}}{\sigma}
\end{aligned}
$$

这里，选择最大的k个奇异值和其对应的奇异向量，就可以得到类似特征值分解的效果

>在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了，也就是说，我们也可以用前k大的奇异值来近似描述矩阵


