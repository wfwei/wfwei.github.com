---
layout: post
title: "Machine Leaning Basics"
category: posts
---

###凸优化问题
$$
min_{w} \indent f(x) \\
s.t. \indent g_i(w)<=0,\indent i=1,2,...,N\\
\indent \indent h_i(x)=0,\indent i=1,2,...,M
$$
其中，f(x)和g(w)是R^N 上的连续可微凸函数，h(w)是R^N 上的仿射函数，即满足h(w)=a*w+b

当目标函数f(x)是二次函数，约束函数g(x)是仿射函数时，上述凸最优化问题成为凸二次规划问题

###结构风险
结构化风险 = 经验风险 + 置信风险
    * 经验风险 =  分类器在给定样本上的误差
    * 置信风险 = 分类器在未知文本上分类的结果的误差

优化理论中，最小化结构化风险，主要是最小化样本上的经验风险

如果考虑防止过拟合，可以添加正则项，同时最小化置信风险

###VC维理论
对一个指示函数集，如果存在h个样本能够被函数集中的函数按所有可能的2^h种形式分开，则称函数集能够把h个样本打散，函数集的VC维就是它能打散的最大样本数目h。

VC维反映了函数集的学习能力，VC维越大则学习机器越复杂，所以VC维又是学习机器复杂程度的一种衡量。

还不是很明白，感觉VC维就像是分类函数中的参数个数相关

###数据倾斜(unbalanced)
* 参与分类的类别样本数量差异很大
* 比如说正类有10，000个样本，而负类只给了100个
* 当考虑离群点而添加惩罚因子的时候，如果存在数据倾斜，不同类别样本的惩罚因子系数应不同，如正比于该类别样本数量

###非度量方法(nonmetric method)
* 数据本身没有数值特征，或者说都是‘语义数据’
* 如：<红色，有光泽，甜味，大>, <黑色，无光泽，甜味，小>...
* 上面的表述方法就是描述语义数据的’属性d-元组‘方法，另外一种常见描述方法是直接串起来,如：红色有光泽甜味大
* 如果对这些数据进行分类，常使用判定树


###岭回归(RidgeRegression) and 套索回归(LASSO)
* 为了控制模型的复杂度，需要对模型中用到的变量进行控制，其中的一种方法就是Regularization
* Regularization一般又分为L1和L2(cost function上的惩罚项)
    * L1为Lasso(Least absolute shrinkage and selection operator),也就是常说的套索
    * L2为Ridge Regression，就是常说的“岭回归”
* 注意L1有sparse的特性，参考[这里](http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/)

###二次规划
* 一类特殊的非线性规划
* 目标函数是二次函数，约束条件是线性的
* 一般形式:
$$ f(x)=(1/2)x^TQx+c^Tx s.t. Ax<=b,Ex=d $$

###正定矩阵
* 正定二次型    对于非零的向量取值恒大于0的二次型 
    * 设$$Q(x)=x^TAx$$是一个二次型，如果对于任何x!=0，都有Q(x)>0，则为正定二次型
    * 正定二次型Q中的矩阵A的特征值全部大于0
    * 半正定二次型 取值大于等于0，A的特征中大于等于0
* 正定矩阵  可以构成正定二次型的矩阵（上面的A）
    * 正定矩阵的特征值都大于0

###最大熵原理
* 学习怪率模型时，在所有可能的概率模型集合中，熵较大的模型较好
* 直观说，首先模型要满足约束条件，不确定的部分使用‘等可能’假设

###最大方差理论
* 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。

###[拉格朗日乘数](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)
* 将有n个变量，k个约束条件的最优化问题转化为n+k个变量的方程组极值问题，其变量不受约束
* 该方法引入新的标量未知数，成为拉格朗日乘数
* 简单例子：
    * 最大(小)化 f(x,y) 受限于 g(x,y)=c
    * 引入拉格朗日乘数lambda,将问题转化为无约束机制问题：k(x,y,lambda)=f(x,y)+lambda\*(g(x,y)-c)

###松弛求解
在实验数据处理和曲线拟合问题中，求解超定方程组非常普遍。这时常常需要退一步，将线性方程组的求解问题改变为求最小误差的问题。形象的说，就是在无法完全满足给定的这些条件的情况下，求一个最接近的解。比较常用的方法是最小二乘法。最小二乘法求解超定问题等价于一个优化问题，或者说最小值问题，即，在不存在x使得Ax-b=0的情况下，我们试图找到这样的x使得|Ax-b|最小，其中|·|表示范数。

###模拟退火(SA,Simulated Annealing)
* 模拟退火是一种贪心算法，但是它的搜索过程引入了随机因素
* 模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出局部的最优解，达到全局的最优解。
* 这个概率随着时间推移逐渐降低，这样才能趋于稳定
* 区别于爬山算法(Hill Climbing)，爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。
* [more](http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html)

###MachineLearning vs DataMining
* A lot of common topics: Clustering, Classification...
* ML focuses more on theory
* DM focuses more on applications, efficiency

###Types of Learning
* Supervised (inductive) learning
    Training data includes desired outputs
* Unsupervised learning
    Training data does not include desired outputs
* Semi-supervised learning
    Training data includes a few desired outputs
* Reinforcement learning
    Rewards from sequence of actions

###幂定律(包括zipf定律和80/20法则) , 马太效应 
* 幂定律，表现为一条斜率为负幂指数的直线,这一线性关系是判断给定的实例中随机变量是否满足幂律的依据
    * zipf定律，把单词出现的频率按由大到小的顺序排列,则每个单词出现的频率与它的名次的常数次幂存在简单的反比关系
    * 80/20法则，20%的人口占据了80%的社会财富
    * 上面二者都是幂定律的表现形式
* 马太效应，描述富者愈富的现象，富者愈富的现象使得人们的收入呈现为幂分布
