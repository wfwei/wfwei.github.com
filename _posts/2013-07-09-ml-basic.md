---
layout: post
title: "Machine Leaning Basics"
category: posts
---

###坐标下降(上升)法
比如优化这个问题: $$\max_\alpha W(\alpha_1, \alpha_2,...,\alpha_n)$$，坐标下降的过程:

$$
\begin{align}\nonumber
Loop~ until & ~convergence: \nonumber \\
      for~  & i=1,2,...,n: \nonumber \\
            & \alpha_i := arg\max_i~W(\alpha_1,...,\alpha_i,...,\alpha_n) \nonumber \\
\end{align}
$$

即，每次只将W看作$$\alpha_i$$的函数，进行求导优化，而其他的$$\alpha_j (j\neq i)$$视为常数

优化过程如下图：

![coordinate-descent](/imame/coordinate-descent.png)

椭圆代表了二次函数的各个等高线，变量数为2，起始坐标是(2,-2)。图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。

###凸优化问题
$$
\begin{align} 
\min_{w} ~& f(x) \\
s.t.~& g_i(w) \leq 0, i=1,2,...,N\\
&h_i(x)=0, i=1,2,...,M
\end{align}
$$
其中，f(x)和g(w)是R^N 上的连续可微凸函数，h(w)是R^N 上的仿射函数，即满足h(w)=a*w+b

当目标函数f(x)是二次函数，约束函数g(x)是仿射函数时，上述凸最优化问题成为凸二次规划问题

###结构风险
结构化风险 = 经验风险 + 置信风险
    * 经验风险 =  分类器在给定样本上的误差
    * 置信风险 = 分类器在未知文本上分类的结果的误差

优化理论中，最小化结构化风险，主要是最小化样本上的经验风险

如果考虑防止过拟合，可以添加正则项，同时最小化置信风险

###VC维理论
对一个指示函数集，如果存在h个样本能够被函数集中的函数按所有可能的2^h种形式分开，则称函数集能够把h个样本打散，函数集的VC维就是它能打散的最大样本数目h。

VC维反映了函数集的学习能力，VC维越大则学习机器越复杂，所以VC维又是学习机器复杂程度的一种衡量。

还不是很明白，感觉VC维就像是分类函数中的参数个数相关

###数据倾斜(unbalanced)
* 参与分类的类别样本数量差异很大
* 比如说正类有10，000个样本，而负类只给了100个
* 当考虑离群点而添加惩罚因子的时候，如果存在数据倾斜，不同类别样本的惩罚因子系数应不同，如正比于该类别样本数量

###非度量方法(nonmetric method)
* 数据本身没有数值特征，或者说都是‘语义数据’
* 如：<红色，有光泽，甜味，大>, <黑色，无光泽，甜味，小>...
* 上面的表述方法就是描述语义数据的’属性d-元组‘方法，另外一种常见描述方法是直接串起来,如：红色有光泽甜味大
* 如果对这些数据进行分类，常使用判定树


###岭回归(RidgeRegression) and 套索回归(LASSO)
* 为了控制模型的复杂度，需要对模型中用到的变量进行控制，其中的一种方法就是Regularization
* Regularization一般又分为L1和L2(cost function上的惩罚项)
    * L1为Lasso(Least absolute shrinkage and selection operator),也就是常说的套索
    * L2为Ridge Regression，就是常说的“岭回归”
* 注意L1有sparse的特性，参考[这里](http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/)

###二次规划
一类特殊的非线性规划，目标函数是二次函数，约束条件是线性的，一般形式:
$$
\begin{align} 
\min f(x) &= \min~(\frac{1}{2}x^TQx+c^Tx) \\
&s.t. Ax<b
\end{align}
$$

###正定矩阵
* 正定二次型    对于非零的向量取值恒大于0的二次型 
    * 设$$Q(x) = x^T A x$$是一个二次型，如果对于任何x!=0，都有Q(x)>0，则为正定二次型
    * 正定二次型Q中的矩阵A的特征值全部大于0
    * 半正定二次型 取值大于等于0，A的特征中大于等于0
* 正定矩阵  可以构成正定二次型的矩阵（上面的A）
    * 正定矩阵的特征值都大于0

###最大熵原理
* 学习怪率模型时，在所有可能的概率模型集合中，熵较大的模型较好
* 直观说，首先模型要满足约束条件，不确定的部分使用‘等可能’假设

###最大方差理论
* 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。

###[拉格朗日乘数](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)
* 将有n个变量，k个约束条件的最优化问题转化为n+k个变量的方程组极值问题，其变量‘不受约束’
* 该方法引入新的标量未知数，称为拉格朗日乘数，要求大于0
* 简单例子：
    * 优化问题：$$\max_x f(x)~~s.t.~g(x)=c$$
    * 拉格朗日函数为: $$k(x, \lambda)=f(x)+\lambda*(g(x)-c)$$ 其中$$\lambda > 0$$
    * 新问题为：$$ \max_{x,\lambda}~k(x, \lambda)~~~where~~\lambda>0 $$

###松弛求解
在实验数据处理和曲线拟合问题中，求解超定方程组非常普遍。这时常常需要退一步，将线性方程组的求解问题改变为__求最小误差的问题__。形象的说，就是在无法完全满足给定的这些条件的情况下，求一个最接近的解

比较常用的方法是最小二乘法。最小二乘法求解超定问题等价于一个优化问题，或者说最小值问题，即，在不存在x使得Ax-b=0的情况下，我们试图找到这样的x使得|Ax-b|最小，其中|·|表示范数。

###模拟退火(SA,Simulated Annealing)
模拟退火是一种贪心算法，但是它的搜索过程引入了随机因素

模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出局部的最优解，达到全局的最优解。

这个概率随着时间推移逐渐降低，这样才能趋于稳定

区别于爬山算法(Hill Climbing)，爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。

[more](http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html)

###MachineLearning vs DataMining

A lot of common topics: Clustering, Classification...

    * ML focuses more on theory
    * DM focuses more on applications, efficiency

###机器学习分类
* 监督学习 Supervised Learning
    训练数据都有标签信息

* 非监督学习 Unsupervised learning
    训练数据不含有标签信息，如聚类问题

* 半监督学习 Semi-Supervised Learning
    训练数据含有部分标签信息，在训练数据十分稀少的情况下，通过利用一些没有类标的数据，提高学习准确率的方法。

* 强化学习 Reinforcement Learning
    Rewards from sequence of actions

* 主动学习 Active Learning
    当含有标签的训练数据缺少的时候，主动学习算法会推荐出比较重要的数据，之后人工标注，之后在训练

###幂定律(包括zipf定律和80/20法则) , 马太效应 
* 幂定律，表现为一条斜率为负幂指数的直线,这一线性关系是判断给定的实例中随机变量是否满足幂律的依据
    * zipf定律，把单词出现的频率按由大到小的顺序排列,则每个单词出现的频率与它的名次的常数次幂存在简单的反比关系
    * 80/20法则，20%的人口占据了80%的社会财富
    * 上面二者都是幂定律的表现形式
* 马太效应，描述富者愈富的现象，富者愈富的现象使得人们的收入呈现为幂分布
