---
layout: post
title: "自然语言处理相关"
category: posts
---

##距离
1. 范数和明可夫斯基距离
    * p范数：p-norm = sum(|Xi-Yi|^p for i=1 to n)^(1/p)
    * 明可夫斯基距离即是p范数
    * p=1,曼哈顿距离
    * p=2，欧氏距离
    * p-->无穷，切比雪夫距离
2. 曼哈顿距离（来源于城市区块距离）
    * dist(X, Y) = sum(|Xi-Yi| for i=1 to n)
3. 切比雪夫距离
    * dist(X, Y) = max(|Xi-Yi| for i=1 to n)
4. 欧氏距离
    * dist(X, Y) = sum(|Xi-Yi|^2 for i=1 to n)^0.5

##相似性
1. 余弦相似度
    * sim(X, Y) = cos(theta) = X*Y/(norm(X)*norm(Y))
    * 修正余弦就是另X=X-avg(X), Y=Y-avg(Y),之后再求余弦
2. Jaccard距离
    * Jaccard(X, Y) = (X交Y)/(X并Y)
3. 皮尔森相关系数
    * 衡量_线性相关_程度
    * Pxy = cov(X, Y)/(cov(X,X)^0.5*cov(X,X)^0.5)
    * cov(X, Y) = sum((Xi-avg(X))*(Yi-avg(Y)) for i=1 to n)/n


##网页正文提取
1. 基于Dom 树 
2. 基于网页分割找正文块
3. 基于标记窗
4. 基于数据挖掘和机器学习
5. 基于逻辑行和最大接纳距离的网页正文提取
6. 基于行块分布函数的通用网页正文抽取
7. 我想可不可以结合stopwords的正文提取

[参考链接](http://code.google.com/p/cx-extractor/)
[参考文档](http://cx-extractor.googlecode.com/files/%E5%9F%BA%E4%BA%8E%E8%A1%8C%E5%9D%97%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%9A%E7%94%A8%E7%BD%91%E9%A1%B5%E6%AD%A3%E6%96%87%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95.pdf)


##相似文本判断算法
1. Shingling方法，抽取多个特征‘词汇’，通过比较特征集合的Jaccard距离(集合的交集/集合的并集)判断文档相似度
    * 抽取特征词汇的方法可以使用‘停用词’，比如取每个停用词及后面k个字符作为特征词汇
    * 抽取特征词汇的方法还可以使用词性，如简单地只使用名词
  
2. I-Match算法，根据统计特性，抽取文档的主要特征进行分析
  认为文档中的高频和低频词汇都不能真正反映文档的内容，所以通过IDF(出现目标词汇的文档的比例)进行过滤?TODO 求证


[参考链接](http://site.douban.com/204776/widget/notes/12599608/note/262427847/)
[参考链接2](http://www.ueoer.org/post/i-match-shingle.html)


##n-gram->language modeling->Markov property->Markov chain->Brownian motion
* n-gram:
在计算语言学和概率论领域，n-gram是从一个给定的文字序列中提取出的ｎ个连续文字。
    
    An n-gram of size 1 is referred to as a "unigram"; 
    size 2 is a "bigram" (or, less commonly, a "digram"); 
    size 3 is a "trigram". Larger sizes are sometimes referred to by the value of n, e.g., "four-gram", "five-gram", and so on.

An n-gram model is a type of probabilistic language model for predicting the next item. More concisely, an n-gram model predicts `xi` based on `xi-(n-1),...,xi-1`. In probability terms, this is P(xi|xi-(n-1),..,xi-1). 

When used for language modeling, independence assumptions are made so that each word depends only on the last n-1 words. This assumption is important because it massively simplifies the problem of learning the language model from data. 

* language modeling
A statistical language model assigns a probability to a sequence of m words `P(w1,...,wm)` by means of a probability distribution.In speech recognition and in data compression, such a model tries to capture the properties of a language, and to predict the next word in a speech sequence.

* Markov property
the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.  

* Markov chain
A system with discrete-time processes with the Markov property is known as a Markov chain.

* Brownian motion
布朗运动(Brownian motion) 过程是一种正态分布的独立增量连续随机过程。1827年英国植物学罗伯特·布朗利用一般的显微镜观察悬浮于水中由花粉所迸裂出之**微粒**时，发现微粒会呈现不规则状的运动，因而称它布朗运动。


