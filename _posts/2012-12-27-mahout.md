---
layout: post
title: "mahout"
---

使用mahout在hadoop机群上进行数据挖掘

* 由于数据是在数据库中，mahout没有提供直接生成seqfile的方法，简单的方法是使用lucene建立索引，因为mahout支持

* 得到向量

    mahout lucene.vector --dir index --output vectors --field doctitle --dictOut dictOut --idField docid
    mahout lucene.vector --dir index --output vectors --field doctitle --dictOut dictOut --idField doctitle --norm 2 --maxDFPercent 20 --minDF 2 -w TFIDF

* 进行聚类

    mahout kmeans -i vectors -o kmeans-result -k 3 -dm org.apache.mahout.common.distance.CosineDistanceMeasure -cd 0.001 -x 2 -ow -cl -c kmeans-clusters
    mahout cvb -i vectors -o lda-res --num_terms 326 -ow --num_topics 20 --doc_topic_output lda-traning --maxIter 10

* 把文件从hadoop fs拷贝到本地

    hdfs -copyToLocal kmeans-result ./kmeans-result

* 显示聚类结果

   mahout clusterdump -o clusteranalyze.txt -p kmeans-result/clusteredPoints -dm org.apache.mahout.common.distance.CosineDistanceMeasure  -i kmeans-result/clusters-0 -d dictOut 


   Before creating the vectors, you need to convert the documents to SequenceFile format. SequenceFile is a hadoop class which allows us to write arbitary key,value pairs into it. The DocumentVectorizer requires the key to be a Text with a unique document id, and value to be the Text content in UTF-8 format.

   mahout下处理的文件必须是SequenceFile格式的，所以需要把txtfile转换成sequenceFile，而聚类必须是向量格式的，mahout提供下面两个命令来将文本转成向量形式

   1. mahout seqdirectory：将文本文件转成SequenceFile文件，SequenceFile文件是一种二制制存储的key-value键值对，对应的源文件是org.apache.mahout.text.SequenceFilesFromDirectory.java
   2. mahout seq2sparse：将SequenceFile转成向量文件，对应的源文件是org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.java

   输出分析：即查看结果

   mahout seqdumper：将SequenceFile文件转成可读的文本形式，对应的源文件是org.apache.mahout.utils.SequenceFileDumper.java
   mahout vectordump：将向量文件转成可读的文本形式，对应的源文件是org.apache.mahout.utils.vectors.VectorDumper.java
   mahout clusterdump：分析最后聚类的输出结果，对应的源文件是org.apache.mahout.utils.clustering.ClusterDumper.java

   运行官网上的mahout kmeas示例，结果文件夹有clusteredPoints，clusters-N，data，用命令mahout seqdumper仔细看了一下结果文件

   clusteredPoints：存放的是最后聚类的结果，将cluster-id和documents-id都展示出来了，用mahout seqdumper读clusteredPoints结果的key-value类型是(IntWritable,WeightedVectorWritable)

   clusters-N：是第N次聚类的结果，其中n为某类的样本数目，c为各类各属性的中心，r为各类属性的半径。 clusters-N结果类型是(Text,Cluster)

   data：存放的是原始数据，这个文件夹下的文件可以用mahout vectordump来读取，原始数据是向量形式的，其它的都只能用mahout seqdumper来读取，向量文件也可以用mahout seqdumper来读取，只是用vectordump读取出来的是数字结果，没有对应的key，用seqdumper读出来的可以看到key，即对应的url，而value读出来的是一个类描述，而不是数组向量

   为什么mahout seqdumper可以读任何SequenceFiles文件呢？看了一下源文件，是因为用的reader.getValueClass().newInstance()去读取的

   clusters-0：读出来的结果(key,value)类型是(Text，Canopy)，我猜应该是随机从原始数据里取出五个点来做初始聚类中心点，看到里面有RandomAccessSparseVector类，而后成的clusters-N读出来的(key，value)都是(Text，Cluster)类型，查了一下Canopy，说Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。

   bin/mahout clusterdump --seqFileDir output/clusters-10 --pointsDir output/clusteredPoints --output /home/test/output
   这个命令会将每一类的点列出来，seqFileDir指向的目录存放的是某类有几个点和该类的质心点及半径，即clusters-N目录，pointsDir指向的是所有输入点归于哪一类，即clusteredPoints目录，两者联合起来，就可以列举出属于同一类的所有点

   注意seqFileDir指向的目录，要是最后一次迭代的结果目录，最后一次迭代的结果才是最终的结果

   mahout Kmeans聚类有两个重要参数：收敛Delta和最大迭代次数.所以有时候改敛时,并还没有达到最大迭代次数  

   关于mahout命令的运行的几点说明

   1.如果没设置HADOOP_HOME，mahout seqdumper的输入路径是本地的目录，而不是HDFS上的目录，如果要查看HDFS上的结果，需要先将文件从HDFS上拷下来，当设置了HADOOP_HOME后，用seqdumper去查看结果时，输入路径则是HDFS上的目录。

   2.读写路径跟源文件中是不是用到hadoop的FileSystem类无关，前面一直理解错了，以为用到这个类，就认为一定是在HDFS上操作，自己写了一个简单的SequenceFile文件读写测试，发现刚开始run as->java application，读写操作是操作的本地目录，如果run as->run on hadoop，则读写操作就在HDFS上，将hadoop停掉后，此时在去run as->java
   application，一直会提示连接不到hdfs，难道一次操作在hdfs上后，后面不管是run as->java application，还是run as->run on hadoop，都必须是在hdsf上吗？有知道原因的大侠，还请赐教！hadoop在启动过后，需要等一段时间，因为还在安全模式下，此时不能在hdfs上创建文件

   3.将(Text，ParseText)->(Text，Text)，这样才能用seq2sparse将文本转成向量，如果不这样的话，必须自己将文本信息用tfidf算法转成对应的数组，然后再由RandomAccessSparseVector类或DenseVector类包装成向量，因为现有的算法中，还没有直接处理文本的，全都要转成数字来度量特征，然后根据某种条件收敛，从这里也可以看出计算机对数学的依赖有多大了，自然语言是不好处理的。

